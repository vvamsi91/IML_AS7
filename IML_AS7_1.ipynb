{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMWGn6mf4Hwp4EWN4lm2gRI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvamsi91/IML_AS7/blob/main/IML_AS7_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31iTJlbmjX_-",
        "outputId": "1d38bdfc-9e86-488b-9c0c-a338f7c83498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.41)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.12)\n",
            "The autotime extension is already loaded. To reload it, use:\n",
            "  %reload_ext autotime\n",
            "time: 4.72 s (started: 2023-12-12 23:33:26 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix, classification_report\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87OAbDkjthhg",
        "outputId": "ac2c2d6b-9d24-44bf-e90d-76dc5082824d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 625 µs (started: 2023-12-12 23:33:31 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_printoptions(edgeitems=2)\n",
        "torch.manual_seed(123)\n",
        "np.random.seed(123)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqlSPReDtnHw",
        "outputId": "754fd406-5f89-4d10-8421-2141cc8ba883"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 559 µs (started: 2023-12-12 23:33:31 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPu4pTFJtxN8",
        "outputId": "795f230c-da70-4c3d-8bb0-693440da5e04"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.85 ms (started: 2023-12-12 23:33:31 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Stack images and calculate mean and std\n",
        "imgs = torch.stack([img_t for img_t, _ in train_dataset], dim=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBDNxLWTuN1P",
        "outputId": "4cef269c-1f48-4689-a8ae-b20b15856f31"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "time: 19.5 s (started: 2023-12-12 23:33:31 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# CIFAR-10 dataset with normalization\n",
        "cifar10 = datasets.CIFAR10(\n",
        "    './data', train=True, download=False,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
        "                             std=[0.2470, 0.2435, 0.2616])\n",
        "    ]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ghs1tlpEuzX6",
        "outputId": "c83eee9b-b061-4e21-f0f6-57d78e0d5173"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.11 s (started: 2023-12-12 23:33:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR-10 validation dataset with normalization\n",
        "cifar10_val = datasets.CIFAR10(\n",
        "     './data', train=False, download=False,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
        "                             std=[0.2470, 0.2435, 0.2616])\n",
        "    ]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1X6S-rYvT_k",
        "outputId": "280d9dfe-a981-4ce1-fa46-87df398e36b1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 441 ms (started: 2023-12-12 23:33:52 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(cifar10, batch_size=32, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(cifar10_val, batch_size=32, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwI2bxsQvw29",
        "outputId": "65389f76-5609-4283-93d0-4566b7beeb99"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 801 µs (started: 2023-12-12 23:33:52 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyk8sv_Xv-a1",
        "outputId": "6e0217ce-4e71-40cd-de13-84ed7bf7cae3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 386 µs (started: 2023-12-12 23:33:52 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.act1 = nn.Tanh()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        self.act2 = nn.Tanh()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "        self.act3 = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pool1(self.act1(self.conv1(x)))\n",
        "        out = self.pool2(self.act2(self.conv2(out)))\n",
        "        out = out.view(-1, 8 * 8 * 8) # <1>\n",
        "        out = self.act3(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "model = Net().to(device)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3St4dAawNJ-",
        "outputId": "3973beb4-342f-4964-9d30-a2460881fbfb"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.73 ms (started: 2023-12-12 23:33:52 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numel_list = [param.numel() for param in model.parameters()]\n",
        "total_parameters = sum(numel_list)\n",
        "\n",
        "print(\"Total number of parameters:\", total_parameters)\n",
        "print(\"Number of parameters per layer:\", numel_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pdc2X5qkwcgV",
        "outputId": "50e8da0d-f23f-409a-87e0-74f88f12716e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 18354\n",
            "Number of parameters per layer: [432, 16, 1152, 8, 16384, 32, 320, 10]\n",
            "time: 4.13 ms (started: 2023-12-12 23:33:52 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        total_loss_train = 0.0\n",
        "\n",
        "        for batch_imgs, batch_labels in train_loader:\n",
        "            batch_imgs = batch_imgs.to(device=device)\n",
        "            batch_labels = batch_labels.to(device=device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            batch_outputs = model(batch_imgs)\n",
        "            batch_loss = loss_fn(batch_outputs, batch_labels)\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss_train += batch_loss.item()\n",
        "\n",
        "        if epoch == 1 or epoch % 2 == 0:\n",
        "            average_loss_train = total_loss_train / len(train_loader)\n",
        "            print('{} Epoch {}, Training loss {:.4f}'.format(\n",
        "                datetime.datetime.now(), epoch, average_loss_train))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL8J7Xb1xWAW",
        "outputId": "7f79ebd0-a8a5-4960-99c3-b6dc6787fbab"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.12 ms (started: 2023-12-12 23:33:52 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the training DataLoader, model, optimizer, and loss function\n",
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
        "model = Net().to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model using the training loop\n",
        "training_loop(\n",
        "    n_epochs=300,\n",
        "    optimizer=optimizer,\n",
        "    model=model,\n",
        "    loss_fn=loss_fn,\n",
        "    train_loader=train_loader,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB-MUe4UxtUJ",
        "outputId": "6b0aa7c7-692e-4720-c406-9240aec49ef7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-12 23:34:17.773583 Epoch 1, Training loss 2.0681\n",
            "2023-12-12 23:34:32.041590 Epoch 2, Training loss 1.7976\n",
            "2023-12-12 23:35:02.212258 Epoch 4, Training loss 1.5253\n",
            "2023-12-12 23:35:31.123083 Epoch 6, Training loss 1.3815\n",
            "2023-12-12 23:35:59.927294 Epoch 8, Training loss 1.2717\n",
            "2023-12-12 23:36:28.583565 Epoch 10, Training loss 1.1939\n",
            "2023-12-12 23:36:59.030233 Epoch 12, Training loss 1.1361\n",
            "2023-12-12 23:37:28.383222 Epoch 14, Training loss 1.0900\n",
            "2023-12-12 23:37:57.413758 Epoch 16, Training loss 1.0554\n",
            "2023-12-12 23:38:27.512185 Epoch 18, Training loss 1.0226\n",
            "2023-12-12 23:38:58.180427 Epoch 20, Training loss 0.9953\n",
            "2023-12-12 23:39:27.323946 Epoch 22, Training loss 0.9691\n",
            "2023-12-12 23:39:56.649290 Epoch 24, Training loss 0.9483\n",
            "2023-12-12 23:40:26.333250 Epoch 26, Training loss 0.9291\n",
            "2023-12-12 23:40:55.851253 Epoch 28, Training loss 0.9137\n",
            "2023-12-12 23:41:26.506516 Epoch 30, Training loss 0.9005\n",
            "2023-12-12 23:41:56.216791 Epoch 32, Training loss 0.8850\n",
            "2023-12-12 23:42:26.786751 Epoch 34, Training loss 0.8752\n",
            "2023-12-12 23:42:56.281160 Epoch 36, Training loss 0.8643\n",
            "2023-12-12 23:43:26.507775 Epoch 38, Training loss 0.8550\n",
            "2023-12-12 23:43:57.757242 Epoch 40, Training loss 0.8447\n",
            "2023-12-12 23:44:25.996470 Epoch 42, Training loss 0.8368\n",
            "2023-12-12 23:44:54.392649 Epoch 44, Training loss 0.8276\n",
            "2023-12-12 23:45:21.946511 Epoch 46, Training loss 0.8200\n",
            "2023-12-12 23:45:49.351638 Epoch 48, Training loss 0.8125\n",
            "2023-12-12 23:46:16.920576 Epoch 50, Training loss 0.8063\n",
            "2023-12-12 23:46:44.972777 Epoch 52, Training loss 0.7997\n",
            "2023-12-12 23:47:11.885612 Epoch 54, Training loss 0.7926\n",
            "2023-12-12 23:47:39.093029 Epoch 56, Training loss 0.7870\n",
            "2023-12-12 23:48:06.865646 Epoch 58, Training loss 0.7815\n",
            "2023-12-12 23:48:34.780853 Epoch 60, Training loss 0.7747\n",
            "2023-12-12 23:49:02.375899 Epoch 62, Training loss 0.7700\n",
            "2023-12-12 23:49:29.601511 Epoch 64, Training loss 0.7643\n",
            "2023-12-12 23:49:56.908499 Epoch 66, Training loss 0.7590\n",
            "2023-12-12 23:50:25.097571 Epoch 68, Training loss 0.7518\n",
            "2023-12-12 23:50:52.653152 Epoch 70, Training loss 0.7475\n",
            "2023-12-12 23:51:20.256672 Epoch 72, Training loss 0.7428\n",
            "2023-12-12 23:51:47.592520 Epoch 74, Training loss 0.7392\n",
            "2023-12-12 23:52:15.507642 Epoch 76, Training loss 0.7350\n",
            "2023-12-12 23:52:42.885304 Epoch 78, Training loss 0.7304\n",
            "2023-12-12 23:53:10.335754 Epoch 80, Training loss 0.7251\n",
            "2023-12-12 23:53:37.690236 Epoch 82, Training loss 0.7224\n",
            "2023-12-12 23:54:05.980132 Epoch 84, Training loss 0.7181\n",
            "2023-12-12 23:54:33.587080 Epoch 86, Training loss 0.7138\n",
            "2023-12-12 23:55:01.088559 Epoch 88, Training loss 0.7110\n",
            "2023-12-12 23:55:28.860401 Epoch 90, Training loss 0.7060\n",
            "2023-12-12 23:55:56.540327 Epoch 92, Training loss 0.7028\n",
            "2023-12-12 23:56:23.598471 Epoch 94, Training loss 0.6997\n",
            "2023-12-12 23:56:51.047226 Epoch 96, Training loss 0.6952\n",
            "2023-12-12 23:57:18.395209 Epoch 98, Training loss 0.6932\n",
            "2023-12-12 23:57:46.812006 Epoch 100, Training loss 0.6893\n",
            "2023-12-12 23:58:14.018296 Epoch 102, Training loss 0.6855\n",
            "2023-12-12 23:58:41.301820 Epoch 104, Training loss 0.6824\n",
            "2023-12-12 23:59:08.364605 Epoch 106, Training loss 0.6804\n",
            "2023-12-12 23:59:35.495187 Epoch 108, Training loss 0.6756\n",
            "2023-12-13 00:00:02.874678 Epoch 110, Training loss 0.6734\n",
            "2023-12-13 00:00:30.044141 Epoch 112, Training loss 0.6723\n",
            "2023-12-13 00:00:57.141619 Epoch 114, Training loss 0.6686\n",
            "2023-12-13 00:01:24.633713 Epoch 116, Training loss 0.6656\n",
            "2023-12-13 00:01:52.032678 Epoch 118, Training loss 0.6633\n",
            "2023-12-13 00:02:19.104956 Epoch 120, Training loss 0.6603\n",
            "2023-12-13 00:02:45.699579 Epoch 122, Training loss 0.6569\n",
            "2023-12-13 00:03:12.971935 Epoch 124, Training loss 0.6555\n",
            "2023-12-13 00:03:40.680476 Epoch 126, Training loss 0.6548\n",
            "2023-12-13 00:04:07.914596 Epoch 128, Training loss 0.6485\n",
            "2023-12-13 00:04:34.913842 Epoch 130, Training loss 0.6468\n",
            "2023-12-13 00:05:02.401613 Epoch 132, Training loss 0.6449\n",
            "2023-12-13 00:05:30.931612 Epoch 134, Training loss 0.6406\n",
            "2023-12-13 00:05:58.277301 Epoch 136, Training loss 0.6396\n",
            "2023-12-13 00:06:25.529664 Epoch 138, Training loss 0.6372\n",
            "2023-12-13 00:06:52.733251 Epoch 140, Training loss 0.6356\n",
            "2023-12-13 00:07:20.461618 Epoch 142, Training loss 0.6319\n",
            "2023-12-13 00:07:47.894568 Epoch 144, Training loss 0.6316\n",
            "2023-12-13 00:08:15.164514 Epoch 146, Training loss 0.6273\n",
            "2023-12-13 00:08:42.163564 Epoch 148, Training loss 0.6251\n",
            "2023-12-13 00:09:09.595544 Epoch 150, Training loss 0.6234\n",
            "2023-12-13 00:09:36.969217 Epoch 152, Training loss 0.6193\n",
            "2023-12-13 00:10:04.185276 Epoch 154, Training loss 0.6195\n",
            "2023-12-13 00:10:31.449203 Epoch 156, Training loss 0.6181\n",
            "2023-12-13 00:10:59.265552 Epoch 158, Training loss 0.6153\n",
            "2023-12-13 00:11:26.721013 Epoch 160, Training loss 0.6128\n",
            "2023-12-13 00:11:53.501235 Epoch 162, Training loss 0.6116\n",
            "2023-12-13 00:12:20.536038 Epoch 164, Training loss 0.6083\n",
            "2023-12-13 00:12:47.675252 Epoch 166, Training loss 0.6085\n",
            "2023-12-13 00:13:15.810162 Epoch 168, Training loss 0.6058\n",
            "2023-12-13 00:13:42.651360 Epoch 170, Training loss 0.6056\n",
            "2023-12-13 00:14:10.132670 Epoch 172, Training loss 0.6026\n",
            "2023-12-13 00:14:37.015625 Epoch 174, Training loss 0.6002\n",
            "2023-12-13 00:15:04.645288 Epoch 176, Training loss 0.5977\n",
            "2023-12-13 00:15:31.815372 Epoch 178, Training loss 0.5967\n",
            "2023-12-13 00:15:59.226157 Epoch 180, Training loss 0.5949\n",
            "2023-12-13 00:16:25.595444 Epoch 182, Training loss 0.5946\n",
            "2023-12-13 00:16:52.268668 Epoch 184, Training loss 0.5921\n",
            "2023-12-13 00:17:19.827268 Epoch 186, Training loss 0.5892\n",
            "2023-12-13 00:17:46.657234 Epoch 188, Training loss 0.5887\n",
            "2023-12-13 00:18:13.125333 Epoch 190, Training loss 0.5859\n",
            "2023-12-13 00:18:39.743988 Epoch 192, Training loss 0.5859\n",
            "2023-12-13 00:19:06.286687 Epoch 194, Training loss 0.5829\n",
            "2023-12-13 00:19:33.369197 Epoch 196, Training loss 0.5826\n",
            "2023-12-13 00:20:00.419040 Epoch 198, Training loss 0.5808\n",
            "2023-12-13 00:20:27.887230 Epoch 200, Training loss 0.5792\n",
            "2023-12-13 00:20:54.977719 Epoch 202, Training loss 0.5807\n",
            "2023-12-13 00:21:21.734841 Epoch 204, Training loss 0.5766\n",
            "2023-12-13 00:21:48.968106 Epoch 206, Training loss 0.5744\n",
            "2023-12-13 00:22:15.947688 Epoch 208, Training loss 0.5728\n",
            "2023-12-13 00:22:42.613911 Epoch 210, Training loss 0.5718\n",
            "2023-12-13 00:23:09.663821 Epoch 212, Training loss 0.5714\n",
            "2023-12-13 00:23:36.829038 Epoch 214, Training loss 0.5702\n",
            "2023-12-13 00:24:03.922103 Epoch 216, Training loss 0.5673\n",
            "2023-12-13 00:24:30.780148 Epoch 218, Training loss 0.5662\n",
            "2023-12-13 00:24:58.004821 Epoch 220, Training loss 0.5655\n",
            "2023-12-13 00:25:25.236571 Epoch 222, Training loss 0.5646\n",
            "2023-12-13 00:25:52.615143 Epoch 224, Training loss 0.5611\n",
            "2023-12-13 00:26:20.152628 Epoch 226, Training loss 0.5630\n",
            "2023-12-13 00:26:47.539253 Epoch 228, Training loss 0.5624\n",
            "2023-12-13 00:27:14.942993 Epoch 230, Training loss 0.5608\n",
            "2023-12-13 00:27:42.610174 Epoch 232, Training loss 0.5587\n",
            "2023-12-13 00:28:10.630150 Epoch 234, Training loss 0.5583\n",
            "2023-12-13 00:28:38.725100 Epoch 236, Training loss 0.5543\n",
            "2023-12-13 00:29:07.168274 Epoch 238, Training loss 0.5553\n",
            "2023-12-13 00:29:34.908108 Epoch 240, Training loss 0.5549\n",
            "2023-12-13 00:30:02.262195 Epoch 242, Training loss 0.5520\n",
            "2023-12-13 00:30:29.900036 Epoch 244, Training loss 0.5515\n",
            "2023-12-13 00:30:57.672630 Epoch 246, Training loss 0.5497\n",
            "2023-12-13 00:31:26.504147 Epoch 248, Training loss 0.5473\n",
            "2023-12-13 00:31:54.295573 Epoch 250, Training loss 0.5483\n",
            "2023-12-13 00:32:22.113553 Epoch 252, Training loss 0.5504\n",
            "2023-12-13 00:32:49.818472 Epoch 254, Training loss 0.5457\n",
            "2023-12-13 00:33:17.719351 Epoch 256, Training loss 0.5473\n",
            "2023-12-13 00:33:45.885835 Epoch 258, Training loss 0.5436\n",
            "2023-12-13 00:34:13.610443 Epoch 260, Training loss 0.5433\n",
            "2023-12-13 00:34:41.625726 Epoch 262, Training loss 0.5442\n",
            "2023-12-13 00:35:09.595829 Epoch 264, Training loss 0.5404\n",
            "2023-12-13 00:35:37.085900 Epoch 266, Training loss 0.5403\n",
            "2023-12-13 00:36:04.362700 Epoch 268, Training loss 0.5389\n",
            "2023-12-13 00:36:32.153496 Epoch 270, Training loss 0.5381\n",
            "2023-12-13 00:36:59.991071 Epoch 272, Training loss 0.5389\n",
            "2023-12-13 00:37:27.313537 Epoch 274, Training loss 0.5352\n",
            "2023-12-13 00:37:54.469001 Epoch 276, Training loss 0.5349\n",
            "2023-12-13 00:38:22.444188 Epoch 278, Training loss 0.5337\n",
            "2023-12-13 00:38:49.889049 Epoch 280, Training loss 0.5322\n",
            "2023-12-13 00:39:17.568824 Epoch 282, Training loss 0.5327\n",
            "2023-12-13 00:39:44.312950 Epoch 284, Training loss 0.5348\n",
            "2023-12-13 00:40:11.455628 Epoch 286, Training loss 0.5318\n",
            "2023-12-13 00:40:39.465215 Epoch 288, Training loss 0.5289\n",
            "2023-12-13 00:41:06.513369 Epoch 290, Training loss 0.5302\n",
            "2023-12-13 00:41:33.646737 Epoch 292, Training loss 0.5293\n",
            "2023-12-13 00:42:01.093995 Epoch 294, Training loss 0.5283\n",
            "2023-12-13 00:42:28.937546 Epoch 296, Training loss 0.5259\n",
            "2023-12-13 00:42:55.916682 Epoch 298, Training loss 0.5267\n",
            "2023-12-13 00:43:23.531950 Epoch 300, Training loss 0.5251\n",
            "time: 1h 9min 30s (started: 2023-12-12 23:33:52 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNfpw8YUyA7P",
        "outputId": "5dba1618-c844-4427-8c05-551960d1454e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 446 µs (started: 2023-12-13 00:43:23 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader configurations\n",
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
        "\n",
        "# Dictionary to store accuracy results\n",
        "all_acc_dict = collections.OrderedDict()\n",
        "\n",
        "def validate(model, train_loader, val_loader):\n",
        "    acc_dict = {}\n",
        "    all_predictions = []\n",
        "    all_expected_labels = []\n",
        "\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in loader:\n",
        "                imgs = imgs.to(device=device)\n",
        "                labels = labels.to(device=device)\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1)  # <1>\n",
        "                total += labels.shape[0]\n",
        "                correct += int((predicted == labels).sum())\n",
        "\n",
        "                all_predictions.extend(predicted.cpu().numpy())\n",
        "                all_expected_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        accuracy = correct / total\n",
        "        print(\"Accuracy {}: {:.2f}\".format(name, accuracy))\n",
        "        acc_dict[name] = accuracy\n",
        "\n",
        "    return acc_dict, all_predictions, all_expected_labels\n",
        "\n",
        "# Call the validate function with the specified loaders\n",
        "all_acc_dict, all_predictions, all_expected_labels = validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECaYqlBiyq3d",
        "outputId": "18594b9c-930e-4f31-e994-26c18a4dfdf7"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.78\n",
            "Accuracy val: 0.61\n",
            "time: 14.8 s (started: 2023-12-13 00:43:23 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy, predictions, expected_labels = validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "SnpuzIsqzByh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy"
      ],
      "metadata": {
        "id": "IY2phYiE9oo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnf_matrix = confusion_matrix(all_predictions, all_expected_labels)\n",
        "cnf_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQNbLJ9nzFmU",
        "outputId": "7a387926-d976-4118-905f-1d5f2c6e9aa4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4597,   75,  214,   73,   92,   22,   13,   50,  199,  118],\n",
              "       [ 104, 5328,   21,   32,   14,   13,   12,   23,  129,  382],\n",
              "       [ 330,   36, 3918,  279,  409,  266,  107,  259,   79,   33],\n",
              "       [ 114,   29,  225, 3435,  266,  861,   94,  297,   53,   75],\n",
              "       [  85,   21,  396,  271, 3985,  232,   72,  515,   36,   35],\n",
              "       [  46,   19,  180,  601,  116, 3896,   35,  342,   20,   34],\n",
              "       [ 115,   95,  876, 1141,  885,  540, 5615,  224,  103,  122],\n",
              "       [  50,   13,   54,   65,  137,  114,   12, 4187,   12,   47],\n",
              "       [ 439,  136,   96,   67,   69,   33,   28,   37, 5254,  189],\n",
              "       [ 120,  248,   20,   36,   27,   23,   12,   66,  115, 4965]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 40.1 ms (started: 2023-12-13 00:43:53 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(predictions, expected_labels, target_names=class_names))"
      ],
      "metadata": {
        "id": "FbkuAaZ50GUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.act1 = nn.Tanh()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(16, 64, kernel_size=3, padding=1)\n",
        "        self.act2 = nn.Tanh()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.conv3 = nn.Conv2d(64, 4, kernel_size=3, padding=1)\n",
        "        self.act3 = nn.Tanh()\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(4 * 4 * 4, 32)\n",
        "        self.act4 = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pool1(self.act1(self.conv1(x)))\n",
        "        out = self.pool2(self.act2(self.conv2(out)))\n",
        "        out = self.pool3(self.act3(self.conv3(out)))\n",
        "        out = out.view(-1, 4 * 4 * 4)\n",
        "        out = self.act4(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5Z0FUgq0RPi",
        "outputId": "9cc7460f-ea30-4d57-b272-09530cf3f292"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.15 ms (started: 2023-12-13 00:43:54 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = Net2().to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1T0Loj0p0iTR",
        "outputId": "2d9a7c68-813f-4767-c845-ab90683e73ae"
      },
      "execution_count": 54,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 4.47 ms (started: 2023-12-13 00:43:54 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
        "model2, optimizer2, loss_fn = Net2().to(device), optim.SGD(Net2().parameters(), lr=1e-2), nn.CrossEntropyLoss()\n",
        "training_loop(300, optimizer2, model2, loss_fn, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Clr2rqaT2Iic",
        "outputId": "7f32ba7b-e51f-4262-bd24-b26a9a2db699"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-13 00:44:08.530193 Epoch 1, Training loss 2.3086\n",
            "2023-12-13 00:44:23.018266 Epoch 2, Training loss 2.3087\n",
            "2023-12-13 00:44:52.291589 Epoch 4, Training loss 2.3086\n",
            "2023-12-13 00:45:22.147918 Epoch 6, Training loss 2.3087\n",
            "2023-12-13 00:45:50.602634 Epoch 8, Training loss 2.3087\n",
            "2023-12-13 00:46:20.212820 Epoch 10, Training loss 2.3087\n",
            "2023-12-13 00:46:49.088391 Epoch 12, Training loss 2.3087\n",
            "2023-12-13 00:47:18.139097 Epoch 14, Training loss 2.3086\n",
            "2023-12-13 00:47:47.819944 Epoch 16, Training loss 2.3087\n",
            "2023-12-13 00:48:17.065250 Epoch 18, Training loss 2.3087\n",
            "2023-12-13 00:48:46.451224 Epoch 20, Training loss 2.3087\n",
            "2023-12-13 00:49:15.429004 Epoch 22, Training loss 2.3086\n",
            "2023-12-13 00:49:44.528088 Epoch 24, Training loss 2.3086\n",
            "2023-12-13 00:50:14.434390 Epoch 26, Training loss 2.3087\n",
            "2023-12-13 00:50:43.422190 Epoch 28, Training loss 2.3087\n",
            "2023-12-13 00:51:13.216901 Epoch 30, Training loss 2.3087\n",
            "2023-12-13 00:51:41.928337 Epoch 32, Training loss 2.3087\n",
            "2023-12-13 00:52:11.073348 Epoch 34, Training loss 2.3087\n",
            "2023-12-13 00:52:40.935973 Epoch 36, Training loss 2.3087\n",
            "2023-12-13 00:53:10.128327 Epoch 38, Training loss 2.3087\n",
            "2023-12-13 00:53:39.805397 Epoch 40, Training loss 2.3087\n",
            "2023-12-13 00:54:08.894355 Epoch 42, Training loss 2.3087\n",
            "2023-12-13 00:54:37.944075 Epoch 44, Training loss 2.3086\n",
            "2023-12-13 00:55:07.479387 Epoch 46, Training loss 2.3087\n",
            "2023-12-13 00:55:37.244678 Epoch 48, Training loss 2.3086\n",
            "2023-12-13 00:56:07.476565 Epoch 50, Training loss 2.3087\n",
            "2023-12-13 00:56:37.477600 Epoch 52, Training loss 2.3087\n",
            "2023-12-13 00:57:08.374650 Epoch 54, Training loss 2.3087\n",
            "2023-12-13 00:57:38.104559 Epoch 56, Training loss 2.3087\n",
            "2023-12-13 00:58:06.981593 Epoch 58, Training loss 2.3087\n",
            "2023-12-13 00:58:37.119097 Epoch 60, Training loss 2.3086\n",
            "2023-12-13 00:59:06.518911 Epoch 62, Training loss 2.3086\n",
            "2023-12-13 00:59:36.549401 Epoch 64, Training loss 2.3086\n",
            "2023-12-13 01:00:05.684985 Epoch 66, Training loss 2.3087\n",
            "2023-12-13 01:00:35.559754 Epoch 68, Training loss 2.3087\n",
            "2023-12-13 01:01:05.093654 Epoch 70, Training loss 2.3087\n",
            "2023-12-13 01:01:34.539878 Epoch 72, Training loss 2.3086\n",
            "2023-12-13 01:02:04.665680 Epoch 74, Training loss 2.3087\n",
            "2023-12-13 01:02:33.811903 Epoch 76, Training loss 2.3087\n",
            "2023-12-13 01:03:04.238690 Epoch 78, Training loss 2.3087\n",
            "2023-12-13 01:03:33.782938 Epoch 80, Training loss 2.3087\n",
            "2023-12-13 01:04:06.574096 Epoch 82, Training loss 2.3087\n",
            "2023-12-13 01:04:36.110311 Epoch 84, Training loss 2.3086\n",
            "2023-12-13 01:05:07.023175 Epoch 86, Training loss 2.3087\n",
            "2023-12-13 01:05:36.213306 Epoch 88, Training loss 2.3087\n",
            "2023-12-13 01:06:05.215540 Epoch 90, Training loss 2.3087\n",
            "2023-12-13 01:06:35.185381 Epoch 92, Training loss 2.3086\n",
            "2023-12-13 01:07:04.547113 Epoch 94, Training loss 2.3086\n",
            "2023-12-13 01:07:34.289819 Epoch 96, Training loss 2.3086\n",
            "2023-12-13 01:08:04.221645 Epoch 98, Training loss 2.3087\n",
            "2023-12-13 01:08:33.885494 Epoch 100, Training loss 2.3087\n",
            "2023-12-13 01:09:03.994191 Epoch 102, Training loss 2.3086\n",
            "2023-12-13 01:09:33.707151 Epoch 104, Training loss 2.3087\n",
            "2023-12-13 01:10:03.873700 Epoch 106, Training loss 2.3087\n",
            "2023-12-13 01:10:33.598472 Epoch 108, Training loss 2.3087\n",
            "2023-12-13 01:11:03.738844 Epoch 110, Training loss 2.3086\n",
            "2023-12-13 01:11:33.515542 Epoch 112, Training loss 2.3087\n",
            "2023-12-13 01:12:03.689015 Epoch 114, Training loss 2.3087\n",
            "2023-12-13 01:12:33.253369 Epoch 116, Training loss 2.3087\n",
            "2023-12-13 01:13:02.717579 Epoch 118, Training loss 2.3087\n",
            "2023-12-13 01:13:32.205904 Epoch 120, Training loss 2.3087\n",
            "2023-12-13 01:14:02.144901 Epoch 122, Training loss 2.3087\n",
            "2023-12-13 01:14:32.508509 Epoch 124, Training loss 2.3087\n",
            "2023-12-13 01:15:02.474287 Epoch 126, Training loss 2.3086\n",
            "2023-12-13 01:15:32.879643 Epoch 128, Training loss 2.3087\n",
            "2023-12-13 01:16:02.570045 Epoch 130, Training loss 2.3087\n",
            "2023-12-13 01:16:32.406914 Epoch 132, Training loss 2.3087\n",
            "2023-12-13 01:17:02.336885 Epoch 134, Training loss 2.3086\n",
            "2023-12-13 01:17:32.770174 Epoch 136, Training loss 2.3086\n",
            "2023-12-13 01:18:03.344609 Epoch 138, Training loss 2.3087\n",
            "2023-12-13 01:18:33.348504 Epoch 140, Training loss 2.3087\n",
            "2023-12-13 01:19:03.783739 Epoch 142, Training loss 2.3087\n",
            "2023-12-13 01:19:33.061882 Epoch 144, Training loss 2.3087\n",
            "2023-12-13 01:20:02.850060 Epoch 146, Training loss 2.3087\n",
            "2023-12-13 01:20:31.994989 Epoch 148, Training loss 2.3087\n",
            "2023-12-13 01:21:01.848070 Epoch 150, Training loss 2.3087\n",
            "2023-12-13 01:21:31.958048 Epoch 152, Training loss 2.3087\n",
            "2023-12-13 01:22:01.486657 Epoch 154, Training loss 2.3087\n",
            "2023-12-13 01:22:31.073789 Epoch 156, Training loss 2.3087\n",
            "2023-12-13 01:23:00.461469 Epoch 158, Training loss 2.3087\n",
            "2023-12-13 01:23:30.359943 Epoch 160, Training loss 2.3086\n",
            "2023-12-13 01:23:59.945873 Epoch 162, Training loss 2.3086\n",
            "2023-12-13 01:24:30.083175 Epoch 164, Training loss 2.3086\n",
            "2023-12-13 01:24:59.758464 Epoch 166, Training loss 2.3087\n",
            "2023-12-13 01:25:28.735134 Epoch 168, Training loss 2.3087\n",
            "2023-12-13 01:25:58.791914 Epoch 170, Training loss 2.3086\n",
            "2023-12-13 01:26:27.934129 Epoch 172, Training loss 2.3087\n",
            "2023-12-13 01:26:57.276225 Epoch 174, Training loss 2.3087\n",
            "2023-12-13 01:27:27.216617 Epoch 176, Training loss 2.3087\n",
            "2023-12-13 01:27:56.621531 Epoch 178, Training loss 2.3087\n",
            "2023-12-13 01:28:25.823870 Epoch 180, Training loss 2.3087\n",
            "2023-12-13 01:28:54.132030 Epoch 182, Training loss 2.3086\n",
            "2023-12-13 01:29:22.574741 Epoch 184, Training loss 2.3087\n",
            "2023-12-13 01:29:51.877491 Epoch 186, Training loss 2.3087\n",
            "2023-12-13 01:30:20.573495 Epoch 188, Training loss 2.3087\n",
            "2023-12-13 01:30:49.148890 Epoch 190, Training loss 2.3087\n",
            "2023-12-13 01:31:18.405979 Epoch 192, Training loss 2.3087\n",
            "2023-12-13 01:31:47.065032 Epoch 194, Training loss 2.3086\n",
            "2023-12-13 01:32:16.413701 Epoch 196, Training loss 2.3087\n",
            "2023-12-13 01:32:45.097011 Epoch 198, Training loss 2.3087\n",
            "2023-12-13 01:33:13.653541 Epoch 200, Training loss 2.3086\n",
            "2023-12-13 01:33:42.827497 Epoch 202, Training loss 2.3087\n",
            "2023-12-13 01:34:11.682136 Epoch 204, Training loss 2.3087\n",
            "2023-12-13 01:34:40.369478 Epoch 206, Training loss 2.3087\n",
            "2023-12-13 01:35:09.718497 Epoch 208, Training loss 2.3087\n",
            "2023-12-13 01:35:38.435725 Epoch 210, Training loss 2.3087\n",
            "2023-12-13 01:36:07.192212 Epoch 212, Training loss 2.3086\n",
            "2023-12-13 01:36:36.018027 Epoch 214, Training loss 2.3087\n",
            "2023-12-13 01:37:04.470399 Epoch 216, Training loss 2.3087\n",
            "2023-12-13 01:37:33.371189 Epoch 218, Training loss 2.3087\n",
            "2023-12-13 01:38:02.450133 Epoch 220, Training loss 2.3087\n",
            "2023-12-13 01:38:30.939297 Epoch 222, Training loss 2.3087\n",
            "2023-12-13 01:39:00.268496 Epoch 224, Training loss 2.3087\n",
            "2023-12-13 01:39:28.982971 Epoch 226, Training loss 2.3087\n",
            "2023-12-13 01:39:58.056828 Epoch 228, Training loss 2.3087\n",
            "2023-12-13 01:40:27.752216 Epoch 230, Training loss 2.3087\n",
            "2023-12-13 01:40:56.917805 Epoch 232, Training loss 2.3087\n",
            "2023-12-13 01:41:26.972260 Epoch 234, Training loss 2.3087\n",
            "2023-12-13 01:41:56.192036 Epoch 236, Training loss 2.3087\n",
            "2023-12-13 01:42:25.433758 Epoch 238, Training loss 2.3086\n",
            "2023-12-13 01:42:55.136190 Epoch 240, Training loss 2.3087\n",
            "2023-12-13 01:43:24.247160 Epoch 242, Training loss 2.3087\n",
            "2023-12-13 01:43:54.181691 Epoch 244, Training loss 2.3087\n",
            "2023-12-13 01:44:24.442857 Epoch 246, Training loss 2.3087\n",
            "2023-12-13 01:44:55.111443 Epoch 248, Training loss 2.3086\n",
            "2023-12-13 01:45:24.714929 Epoch 250, Training loss 2.3087\n",
            "2023-12-13 01:45:55.111731 Epoch 252, Training loss 2.3087\n",
            "2023-12-13 01:46:24.775131 Epoch 254, Training loss 2.3087\n",
            "2023-12-13 01:46:54.418947 Epoch 256, Training loss 2.3086\n",
            "2023-12-13 01:47:24.998334 Epoch 258, Training loss 2.3087\n",
            "2023-12-13 01:47:54.518984 Epoch 260, Training loss 2.3087\n",
            "2023-12-13 01:48:24.661128 Epoch 262, Training loss 2.3086\n",
            "2023-12-13 01:48:54.631599 Epoch 264, Training loss 2.3087\n",
            "2023-12-13 01:49:24.602652 Epoch 266, Training loss 2.3087\n",
            "2023-12-13 01:49:53.961198 Epoch 268, Training loss 2.3087\n",
            "2023-12-13 01:50:23.405043 Epoch 270, Training loss 2.3087\n",
            "2023-12-13 01:50:53.707989 Epoch 272, Training loss 2.3087\n",
            "2023-12-13 01:51:23.186990 Epoch 274, Training loss 2.3086\n",
            "2023-12-13 01:51:53.153317 Epoch 276, Training loss 2.3087\n",
            "2023-12-13 01:52:22.832560 Epoch 278, Training loss 2.3087\n",
            "2023-12-13 01:52:52.928999 Epoch 280, Training loss 2.3087\n",
            "2023-12-13 01:53:22.730979 Epoch 282, Training loss 2.3087\n",
            "2023-12-13 01:53:52.043205 Epoch 284, Training loss 2.3087\n",
            "2023-12-13 01:54:21.918742 Epoch 286, Training loss 2.3087\n",
            "2023-12-13 01:54:51.407205 Epoch 288, Training loss 2.3087\n",
            "2023-12-13 01:55:21.254305 Epoch 290, Training loss 2.3087\n",
            "2023-12-13 01:55:51.100072 Epoch 292, Training loss 2.3087\n",
            "2023-12-13 01:56:21.019750 Epoch 294, Training loss 2.3087\n",
            "2023-12-13 01:56:50.821535 Epoch 296, Training loss 2.3086\n",
            "2023-12-13 01:57:20.309366 Epoch 298, Training loss 2.3087\n",
            "2023-12-13 01:57:50.473598 Epoch 300, Training loss 2.3087\n",
            "time: 1h 13min 56s (started: 2023-12-13 00:43:54 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
        "accuracy2, predictions2, expected_labels2 = validate(model2, train_loader, val_loader)\n",
        "accuracy2"
      ],
      "metadata": {
        "id": "jaYUeFxw4V0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnf_matrix2 = confusion_matrix(predictions2, expected_labels2)\n",
        "cnf_matrix2"
      ],
      "metadata": {
        "id": "yItitIXj4j1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(predictions2, expected_labels2, target_names=class_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pw2co5DL5Gh7",
        "outputId": "4e5eecd5-510b-474a-9aff-fd7976f5399a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane       0.00      0.00      0.00         0\n",
            "  automobile       0.00      0.00      0.00         0\n",
            "        bird       1.00      0.10      0.18     59972\n",
            "         cat       0.00      0.00      0.00         0\n",
            "        deer       0.00      0.00      0.00         0\n",
            "         dog       0.00      0.25      0.00        28\n",
            "        frog       0.00      0.00      0.00         0\n",
            "       horse       0.00      0.00      0.00         0\n",
            "        ship       0.00      0.00      0.00         0\n",
            "       truck       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.10     60000\n",
            "   macro avg       0.10      0.04      0.02     60000\n",
            "weighted avg       1.00      0.10      0.18     60000\n",
            "\n",
            "time: 171 ms (started: 2023-12-13 02:00:25 +00:00)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}