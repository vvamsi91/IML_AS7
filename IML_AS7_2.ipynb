{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvamsi91/IML_AS7/blob/main/IML_AS7_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import datetime\n"
      ],
      "metadata": {
        "id": "gu46Wy68ATOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_printoptions(edgeitems=2)\n",
        "torch.manual_seed(123)\n",
        "np.random.seed(123)\n"
      ],
      "metadata": {
        "id": "NGLVZ_FLAyXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P5C1MbRA833",
        "outputId": "72352b45-44d1-40f3-84d1-61fef26a04e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ehrk3M39CFEm",
        "outputId": "9831596d-2c19-4864-95e9-ab7cc17c4aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 80582378.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
        "cifar10 = datasets.CIFAR10('./data', train=True, download=False, transform=transform)\n"
      ],
      "metadata": {
        "id": "ST-pCBIoCX_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10_val = datasets.CIFAR10('./data', train=False, download=False,\n",
        "                               transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))]))\n"
      ],
      "metadata": {
        "id": "DOWtOJRxDs5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(cifar10, batch_size=32, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(cifar10_val, batch_size=32, shuffle=False, num_workers=2)\n",
        "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKe2d9wUD51_",
        "outputId": "f540ef0c-26c8-463a-9de2-3f35985527f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device=device)\n",
        "            labels = labels.to(device=device)\n",
        "            outputs = model(imgs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_train += loss.item()\n",
        "\n",
        "        if epoch == 1 or epoch % 2 == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch,\n",
        "                loss_train / len(train_loader)))"
      ],
      "metadata": {
        "id": "JsHV0fqwEIM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, n_chans):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias=False)\n",
        "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
        "        nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
        "        nn.init.constant_(self.batch_norm.weight, 0.5)\n",
        "        nn.init.zeros_(self.batch_norm.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.relu(self.batch_norm(self.conv(x))) + x\n"
      ],
      "metadata": {
        "id": "YfobUP7aEqk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NetResDeep(nn.Module):\n",
        "    def __init__(self, n_chans1=32, n_blocks=10):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.resblocks = nn.Sequential(\n",
        "            *(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
        "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "        out = self.resblocks(out)\n",
        "        out = F.max_pool2d(out, 2)\n",
        "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "QCaTyM6NE8ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0_XFOgfFEsu",
        "outputId": "5c23bbb6-0aa2-41d5-9ea4-58f857cab794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NetResDeep(n_chans1=32, n_blocks=10).to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=3e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "training_loop(\n",
        "    n_epochs = 300,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdJnaCZhFJLv",
        "outputId": "4148d048-4d27-4c46-e388-5cc644154fd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-12 23:34:56.526688 Epoch 1, Training loss 1.578044022647372\n",
            "2023-12-12 23:35:27.492537 Epoch 2, Training loss 1.2774697199358973\n",
            "2023-12-12 23:36:32.293571 Epoch 4, Training loss 1.0684302118597928\n",
            "2023-12-12 23:37:34.856660 Epoch 6, Training loss 0.9435079231982192\n",
            "2023-12-12 23:38:38.427915 Epoch 8, Training loss 0.8640125250671433\n",
            "2023-12-12 23:39:41.109520 Epoch 10, Training loss 0.8033782051148052\n",
            "2023-12-12 23:40:45.517788 Epoch 12, Training loss 0.7533268784378403\n",
            "2023-12-12 23:41:48.510045 Epoch 14, Training loss 0.7069631824490358\n",
            "2023-12-12 23:42:52.438472 Epoch 16, Training loss 0.670059420039695\n",
            "2023-12-12 23:43:55.241447 Epoch 18, Training loss 0.6337188809862216\n",
            "2023-12-12 23:44:57.774047 Epoch 20, Training loss 0.5957012096049308\n",
            "2023-12-12 23:45:59.431344 Epoch 22, Training loss 0.5671416467874384\n",
            "2023-12-12 23:47:00.945944 Epoch 24, Training loss 0.5380308904607023\n",
            "2023-12-12 23:48:02.688664 Epoch 26, Training loss 0.5092968434924814\n",
            "2023-12-12 23:49:03.341639 Epoch 28, Training loss 0.4845182262146542\n",
            "2023-12-12 23:50:04.722702 Epoch 30, Training loss 0.4553109216226726\n",
            "2023-12-12 23:51:06.053999 Epoch 32, Training loss 0.4371406581021621\n",
            "2023-12-12 23:52:07.159312 Epoch 34, Training loss 0.4116578655304317\n",
            "2023-12-12 23:53:08.524253 Epoch 36, Training loss 0.3885237137076188\n",
            "2023-12-12 23:54:10.229025 Epoch 38, Training loss 0.3697478868977732\n",
            "2023-12-12 23:55:12.523496 Epoch 40, Training loss 0.34832419615455795\n",
            "2023-12-12 23:56:13.096124 Epoch 42, Training loss 0.3309368825209537\n",
            "2023-12-12 23:57:14.365408 Epoch 44, Training loss 0.30894808262176643\n",
            "2023-12-12 23:58:17.197767 Epoch 46, Training loss 0.29903752980032794\n",
            "2023-12-12 23:59:19.273397 Epoch 48, Training loss 0.28292824497407093\n",
            "2023-12-13 00:00:21.763324 Epoch 50, Training loss 0.26470092633621134\n",
            "2023-12-13 00:01:23.073933 Epoch 52, Training loss 0.24961829264377144\n",
            "2023-12-13 00:02:24.764472 Epoch 54, Training loss 0.23846471402377023\n",
            "2023-12-13 00:03:26.178350 Epoch 56, Training loss 0.2276723424529255\n",
            "2023-12-13 00:04:27.552720 Epoch 58, Training loss 0.21704835193833516\n",
            "2023-12-13 00:05:28.416771 Epoch 60, Training loss 0.2045932295997356\n",
            "2023-12-13 00:06:29.606948 Epoch 62, Training loss 0.19715112037253604\n",
            "2023-12-13 00:07:32.070318 Epoch 64, Training loss 0.1839462614827387\n",
            "2023-12-13 00:08:34.530674 Epoch 66, Training loss 0.18027733815911387\n",
            "2023-12-13 00:09:36.695744 Epoch 68, Training loss 0.17620679240027873\n",
            "2023-12-13 00:10:39.150595 Epoch 70, Training loss 0.16373911593258333\n",
            "2023-12-13 00:11:40.941055 Epoch 72, Training loss 0.1521454211896475\n",
            "2023-12-13 00:12:43.825895 Epoch 74, Training loss 0.14983267218568103\n",
            "2023-12-13 00:13:46.613860 Epoch 76, Training loss 0.138958377879225\n",
            "2023-12-13 00:14:49.350926 Epoch 78, Training loss 0.14688206901589096\n",
            "2023-12-13 00:15:51.414098 Epoch 80, Training loss 0.13301729219438305\n",
            "2023-12-13 00:16:52.429248 Epoch 82, Training loss 0.12373978863854583\n",
            "2023-12-13 00:17:55.164711 Epoch 84, Training loss 0.12617750362578306\n",
            "2023-12-13 00:18:57.267388 Epoch 86, Training loss 0.11268119631044361\n",
            "2023-12-13 00:19:59.460150 Epoch 88, Training loss 0.1108000941204547\n",
            "2023-12-13 00:21:01.810151 Epoch 90, Training loss 0.09700312145156828\n",
            "2023-12-13 00:22:02.904121 Epoch 92, Training loss 0.12361729768300292\n",
            "2023-12-13 00:23:03.449227 Epoch 94, Training loss 0.09471321224442952\n",
            "2023-12-13 00:24:03.714968 Epoch 96, Training loss 0.09686036956492365\n",
            "2023-12-13 00:25:04.230343 Epoch 98, Training loss 0.09450547488943166\n",
            "2023-12-13 00:26:04.932917 Epoch 100, Training loss 0.08628008192396465\n",
            "2023-12-13 00:27:05.497034 Epoch 102, Training loss 0.08521552222445118\n",
            "2023-12-13 00:28:07.368974 Epoch 104, Training loss 0.08819299035852207\n",
            "2023-12-13 00:29:07.902876 Epoch 106, Training loss 0.09147211696044795\n",
            "2023-12-13 00:30:08.720370 Epoch 108, Training loss 0.09477241371642985\n",
            "2023-12-13 00:31:09.582203 Epoch 110, Training loss 0.06867516281632009\n",
            "2023-12-13 00:32:10.087765 Epoch 112, Training loss 0.06534520681981061\n",
            "2023-12-13 00:33:10.297347 Epoch 114, Training loss 0.06625027497486263\n",
            "2023-12-13 00:34:10.526251 Epoch 116, Training loss 0.0741172589017412\n",
            "2023-12-13 00:35:11.775759 Epoch 118, Training loss 0.08394650316881212\n",
            "2023-12-13 00:36:13.819759 Epoch 120, Training loss 0.07798285194011542\n",
            "2023-12-13 00:37:15.843307 Epoch 122, Training loss 0.07507893621393238\n",
            "2023-12-13 00:38:17.841380 Epoch 124, Training loss 0.05894785409733523\n",
            "2023-12-13 00:39:19.937424 Epoch 126, Training loss 0.05407027863855003\n",
            "2023-12-13 00:40:21.541970 Epoch 128, Training loss 0.07415821798689787\n",
            "2023-12-13 00:41:22.936494 Epoch 130, Training loss 0.06019687743552723\n",
            "2023-12-13 00:42:25.181980 Epoch 132, Training loss 0.05215247360329839\n",
            "2023-12-13 00:43:26.935584 Epoch 134, Training loss 0.0517068272663959\n",
            "2023-12-13 00:44:28.963272 Epoch 136, Training loss 0.06645800497886228\n",
            "2023-12-13 00:45:30.630171 Epoch 138, Training loss 0.06148545340712642\n",
            "2023-12-13 00:46:31.305650 Epoch 140, Training loss 0.043543671966247796\n",
            "2023-12-13 00:47:32.051955 Epoch 142, Training loss 0.04375420000756382\n",
            "2023-12-13 00:48:33.494402 Epoch 144, Training loss 0.0764258217391603\n",
            "2023-12-13 00:49:35.737822 Epoch 146, Training loss 0.06868575870632421\n",
            "2023-12-13 00:50:37.569368 Epoch 148, Training loss 0.047783366977742894\n",
            "2023-12-13 00:51:38.474284 Epoch 150, Training loss 0.04845309836022764\n",
            "2023-12-13 00:52:39.943155 Epoch 152, Training loss 0.04003255575477205\n",
            "2023-12-13 00:53:41.770962 Epoch 154, Training loss 0.0388888268285313\n",
            "2023-12-13 00:54:42.672574 Epoch 156, Training loss 0.05199810791345744\n",
            "2023-12-13 00:55:44.705759 Epoch 158, Training loss 0.050315804910422327\n",
            "2023-12-13 00:56:46.372330 Epoch 160, Training loss 0.04537299366108684\n",
            "2023-12-13 00:57:48.097554 Epoch 162, Training loss 0.03676449652683089\n",
            "2023-12-13 00:58:49.793266 Epoch 164, Training loss 0.028874790019723505\n",
            "2023-12-13 00:59:51.835228 Epoch 166, Training loss 0.03911193313394395\n",
            "2023-12-13 01:00:53.696700 Epoch 168, Training loss 0.04093344089285861\n",
            "2023-12-13 01:01:54.670863 Epoch 170, Training loss 0.040701146496777356\n",
            "2023-12-13 01:02:55.043885 Epoch 172, Training loss 0.05068751329872321\n",
            "2023-12-13 01:03:57.512865 Epoch 174, Training loss 0.05081136454660034\n",
            "2023-12-13 01:04:59.290777 Epoch 176, Training loss 0.04660731069406044\n",
            "2023-12-13 01:06:00.993522 Epoch 178, Training loss 0.04143669360724576\n",
            "2023-12-13 01:07:02.643975 Epoch 180, Training loss 0.038237186843213984\n",
            "2023-12-13 01:08:04.412256 Epoch 182, Training loss 0.04735084853477396\n",
            "2023-12-13 01:09:05.210855 Epoch 184, Training loss 0.036949294580135905\n",
            "2023-12-13 01:10:06.748273 Epoch 186, Training loss 0.037561170910766446\n",
            "2023-12-13 01:11:08.481452 Epoch 188, Training loss 0.04377102868993874\n",
            "2023-12-13 01:12:10.064843 Epoch 190, Training loss 0.038585462627695445\n",
            "2023-12-13 01:13:11.463865 Epoch 192, Training loss 0.02691587232341078\n",
            "2023-12-13 01:14:12.491499 Epoch 194, Training loss 0.04071961141774654\n",
            "2023-12-13 01:15:13.765952 Epoch 196, Training loss 0.0217581363934618\n",
            "2023-12-13 01:16:14.861718 Epoch 198, Training loss 0.026280496580447673\n",
            "2023-12-13 01:17:16.629555 Epoch 200, Training loss 0.035617834562870536\n",
            "2023-12-13 01:18:18.272103 Epoch 202, Training loss 0.02072477901161347\n",
            "2023-12-13 01:19:20.004199 Epoch 204, Training loss 0.026034104061880274\n",
            "2023-12-13 01:20:21.946492 Epoch 206, Training loss 0.02928658492288652\n",
            "2023-12-13 01:21:23.717028 Epoch 208, Training loss 0.04469675174811655\n",
            "2023-12-13 01:22:25.506753 Epoch 210, Training loss 0.03936360096947259\n",
            "2023-12-13 01:23:27.420889 Epoch 212, Training loss 0.047606437845246444\n",
            "2023-12-13 01:24:29.496301 Epoch 214, Training loss 0.038680154064982604\n",
            "2023-12-13 01:25:31.060484 Epoch 216, Training loss 0.03421261076551825\n",
            "2023-12-13 01:26:33.290468 Epoch 218, Training loss 0.022359087959754335\n",
            "2023-12-13 01:27:35.283615 Epoch 220, Training loss 0.03335893595396145\n",
            "2023-12-13 01:28:37.192824 Epoch 222, Training loss 0.019090157795273704\n",
            "2023-12-13 01:29:40.059064 Epoch 224, Training loss 0.02100669758797357\n",
            "2023-12-13 01:30:42.275501 Epoch 226, Training loss 0.01971711548092732\n",
            "2023-12-13 01:31:46.005293 Epoch 228, Training loss 0.013210989887606162\n",
            "2023-12-13 01:32:48.526222 Epoch 230, Training loss 0.020038680327758875\n",
            "2023-12-13 01:33:50.359229 Epoch 232, Training loss 0.01648583673715995\n",
            "2023-12-13 01:34:51.862435 Epoch 234, Training loss 0.02010514781244974\n",
            "2023-12-13 01:35:52.270233 Epoch 236, Training loss 0.03444521487266793\n",
            "2023-12-13 01:36:54.121009 Epoch 238, Training loss 0.03888690032630317\n",
            "2023-12-13 01:37:55.669551 Epoch 240, Training loss 0.025164866356687094\n",
            "2023-12-13 01:38:57.082169 Epoch 242, Training loss 0.028009021716650432\n",
            "2023-12-13 01:39:58.761021 Epoch 244, Training loss 0.02013430201470668\n",
            "2023-12-13 01:41:00.870033 Epoch 246, Training loss 0.024425294988841606\n",
            "2023-12-13 01:42:02.328976 Epoch 248, Training loss 0.017773169356560693\n",
            "2023-12-13 01:43:04.699709 Epoch 250, Training loss 0.023344370513420257\n",
            "2023-12-13 01:44:06.897988 Epoch 252, Training loss 0.02089590357425018\n",
            "2023-12-13 01:45:08.839488 Epoch 254, Training loss 0.021249288812819383\n",
            "2023-12-13 01:46:10.204242 Epoch 256, Training loss 0.0260120574547537\n",
            "2023-12-13 01:47:11.004839 Epoch 258, Training loss 0.019262380613239104\n",
            "2023-12-13 01:48:12.759662 Epoch 260, Training loss 0.014302415582225093\n",
            "2023-12-13 01:49:14.414329 Epoch 262, Training loss 0.03732848784345808\n",
            "2023-12-13 01:50:15.436188 Epoch 264, Training loss 0.018647475581112054\n",
            "2023-12-13 01:51:17.358271 Epoch 266, Training loss 0.02659997293533672\n",
            "2023-12-13 01:52:18.479413 Epoch 268, Training loss 0.01743000937121516\n",
            "2023-12-13 01:53:19.370876 Epoch 270, Training loss 0.038487558292548824\n",
            "2023-12-13 01:54:21.120948 Epoch 272, Training loss 0.034626614963852546\n",
            "2023-12-13 01:55:23.264725 Epoch 274, Training loss 0.034514315437473055\n",
            "2023-12-13 01:56:24.453330 Epoch 276, Training loss 0.021553561795688483\n",
            "2023-12-13 01:57:26.174456 Epoch 278, Training loss 0.029316374052485352\n",
            "2023-12-13 01:58:28.295715 Epoch 280, Training loss 0.021828507931458983\n",
            "2023-12-13 01:59:29.957369 Epoch 282, Training loss 0.023497476053228047\n",
            "2023-12-13 02:00:31.614382 Epoch 284, Training loss 0.017015073087907624\n",
            "2023-12-13 02:01:33.344630 Epoch 286, Training loss 0.019461717684192623\n",
            "2023-12-13 02:02:35.305103 Epoch 288, Training loss 0.022105817057919724\n",
            "2023-12-13 02:03:36.577174 Epoch 290, Training loss 0.02364829102356294\n",
            "2023-12-13 02:04:36.924726 Epoch 292, Training loss 0.021311142737119617\n",
            "2023-12-13 02:05:38.414291 Epoch 294, Training loss 0.02323270168662796\n",
            "2023-12-13 02:06:40.210943 Epoch 296, Training loss 0.027499508730318888\n",
            "2023-12-13 02:07:41.708311 Epoch 298, Training loss 0.021928575185683304\n",
            "2023-12-13 02:08:43.673800 Epoch 300, Training loss 0.025813420696416758\n",
            "time: 2h 34min 31s (started: 2023-12-12 23:34:12 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, train_loader, val_loader):\n",
        "    accdict = {}\n",
        "\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        predictions, exp_labels = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in loader:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1)\n",
        "                total += labels.shape[0]\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                predictions.extend(predicted.cpu().numpy())\n",
        "                exp_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        accuracy = correct / total\n",
        "        print(f\"Accuracy {name}: {accuracy:.2f}\")\n",
        "        accdict[name] = accuracy\n",
        "\n",
        "    return accdict, predictions, exp_labels\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "-11MS-gWFb6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy, predictions, expected_labels = validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82SIgC3DGU5J",
        "outputId": "8919e95f-047d-49a1-e29a-a858d316e03d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.99\n",
            "Accuracy val: 0.66\n",
            "time: 22.6 s (started: 2023-12-13 02:08:43 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
        "\n",
        "precision, recall = precision_score(predictions, expected_labels, average='macro'), recall_score(predictions, expected_labels, average='macro')\n",
        "cnf_matrix = confusion_matrix(predictions, expected_labels)\n"
      ],
      "metadata": {
        "id": "eQIjUdcoGwUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(predictions, expected_labels, target_names=class_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOulbZEuH2aW",
        "outputId": "ab093d83-f8cb-4ad8-a0d9-50038822e620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane       0.72      0.69      0.70      1052\n",
            "  automobile       0.75      0.83      0.79       896\n",
            "        bird       0.57      0.51      0.53      1116\n",
            "         cat       0.48      0.46      0.47      1037\n",
            "        deer       0.64      0.61      0.62      1047\n",
            "         dog       0.52      0.57      0.54       923\n",
            "        frog       0.68      0.78      0.73       873\n",
            "       horse       0.71      0.73      0.72       975\n",
            "        ship       0.77      0.79      0.78       968\n",
            "       truck       0.81      0.72      0.76      1113\n",
            "\n",
            "    accuracy                           0.66     10000\n",
            "   macro avg       0.66      0.67      0.67     10000\n",
            "weighted avg       0.66      0.66      0.66     10000\n",
            "\n",
            "time: 52.5 ms (started: 2023-12-13 02:09:07 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NetDropout(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.conv1_dropout = nn.Dropout2d(p=0.3)\n",
        "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
        "        self.conv2_dropout = nn.Dropout2d(p=0.3)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(8 * 8 * n_chans1 // 2, 32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = self.conv1_dropout(out)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = self.conv2_dropout(out)\n",
        "        return self.fc(out.view(-1, 8 * 8 * self.n_chans1 // 2))\n"
      ],
      "metadata": {
        "id": "XKDuDtrVH4ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dropout = NetDropout(n_chans1=32).to(device=device)\n",
        "optimizer_dropout = optim.SGD(model_dropout.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 300,\n",
        "    optimizer = optimizer_dropout,\n",
        "    model = model_dropout,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9C2anmjIVL1",
        "outputId": "20672a37-6439-4a0d-c6ce-7fe627c62f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-13 02:09:23.387425 Epoch 1, Training loss 2.0211501491953956\n",
            "2023-12-13 02:09:38.183454 Epoch 2, Training loss 1.7908894675771903\n",
            "2023-12-13 02:10:08.669378 Epoch 4, Training loss 1.563987286194511\n",
            "2023-12-13 02:10:38.959994 Epoch 6, Training loss 1.4745130110579683\n",
            "2023-12-13 02:11:10.133252 Epoch 8, Training loss 1.4042231009134551\n",
            "2023-12-13 02:11:40.428335 Epoch 10, Training loss 1.3402042797459361\n",
            "2023-12-13 02:12:11.413832 Epoch 12, Training loss 1.2961306644370183\n",
            "2023-12-13 02:12:41.722395 Epoch 14, Training loss 1.259879713015788\n",
            "2023-12-13 02:13:13.251650 Epoch 16, Training loss 1.2310235088743517\n",
            "2023-12-13 02:13:43.756423 Epoch 18, Training loss 1.2080232135932465\n",
            "2023-12-13 02:14:15.067404 Epoch 20, Training loss 1.186558669028075\n",
            "2023-12-13 02:14:45.552902 Epoch 22, Training loss 1.1730980440173917\n",
            "2023-12-13 02:15:16.358477 Epoch 24, Training loss 1.1533615790364686\n",
            "2023-12-13 02:15:46.880985 Epoch 26, Training loss 1.1397344309198276\n",
            "2023-12-13 02:16:17.955112 Epoch 28, Training loss 1.1273841820561978\n",
            "2023-12-13 02:16:49.089324 Epoch 30, Training loss 1.1113284176877698\n",
            "2023-12-13 02:17:20.265423 Epoch 32, Training loss 1.0975609985001558\n",
            "2023-12-13 02:17:51.827513 Epoch 34, Training loss 1.0862776800951994\n",
            "2023-12-13 02:18:22.959356 Epoch 36, Training loss 1.079797618834259\n",
            "2023-12-13 02:18:54.543978 Epoch 38, Training loss 1.064483254157064\n",
            "2023-12-13 02:19:25.743265 Epoch 40, Training loss 1.0626859567354403\n",
            "2023-12-13 02:19:56.179370 Epoch 42, Training loss 1.0511827994795406\n",
            "2023-12-13 02:20:26.265246 Epoch 44, Training loss 1.0449048523098001\n",
            "2023-12-13 02:20:57.486648 Epoch 46, Training loss 1.0371223017382805\n",
            "2023-12-13 02:21:28.336417 Epoch 48, Training loss 1.0329275199824282\n",
            "2023-12-13 02:21:59.491901 Epoch 50, Training loss 1.0265617601554413\n",
            "2023-12-13 02:22:30.323956 Epoch 52, Training loss 1.0194970777882335\n",
            "2023-12-13 02:23:01.685150 Epoch 54, Training loss 1.017082819014864\n",
            "2023-12-13 02:23:32.776407 Epoch 56, Training loss 1.009828996246733\n",
            "2023-12-13 02:24:04.117136 Epoch 58, Training loss 1.004884359248154\n",
            "2023-12-13 02:24:36.534348 Epoch 60, Training loss 0.9990148793553453\n",
            "2023-12-13 02:25:07.260006 Epoch 62, Training loss 0.9959562144163624\n",
            "2023-12-13 02:25:37.857603 Epoch 64, Training loss 0.9903986072906142\n",
            "2023-12-13 02:26:07.782176 Epoch 66, Training loss 0.9850526320202576\n",
            "2023-12-13 02:26:39.276281 Epoch 68, Training loss 0.9827434557783025\n",
            "2023-12-13 02:27:10.225944 Epoch 70, Training loss 0.9757778736026695\n",
            "2023-12-13 02:27:41.844043 Epoch 72, Training loss 0.9718107867728719\n",
            "2023-12-13 02:28:12.443714 Epoch 74, Training loss 0.9725868181346933\n",
            "2023-12-13 02:28:44.109817 Epoch 76, Training loss 0.9691897742736065\n",
            "2023-12-13 02:29:14.892669 Epoch 78, Training loss 0.9694678547894559\n",
            "2023-12-13 02:29:46.256318 Epoch 80, Training loss 0.9660944977532262\n",
            "2023-12-13 02:30:17.765376 Epoch 82, Training loss 0.9630342917826474\n",
            "2023-12-13 02:30:48.090804 Epoch 84, Training loss 0.9603362917290319\n",
            "2023-12-13 02:31:18.639459 Epoch 86, Training loss 0.9572572029002792\n",
            "2023-12-13 02:31:48.936779 Epoch 88, Training loss 0.9521126606885124\n",
            "2023-12-13 02:32:20.410473 Epoch 90, Training loss 0.9541208680023623\n",
            "2023-12-13 02:32:51.062030 Epoch 92, Training loss 0.9576957716661341\n",
            "2023-12-13 02:33:22.614098 Epoch 94, Training loss 0.9460620412893612\n",
            "2023-12-13 02:33:53.461275 Epoch 96, Training loss 0.9463747328961901\n",
            "2023-12-13 02:34:24.802925 Epoch 98, Training loss 0.9441576151134413\n",
            "2023-12-13 02:34:55.664912 Epoch 100, Training loss 0.9413916054741501\n",
            "2023-12-13 02:35:27.085152 Epoch 102, Training loss 0.9405401707305323\n",
            "2023-12-13 02:35:58.215832 Epoch 104, Training loss 0.943800795718532\n",
            "2023-12-13 02:36:29.077030 Epoch 106, Training loss 0.9370599498834147\n",
            "2023-12-13 02:36:59.167502 Epoch 108, Training loss 0.9359104822358817\n",
            "2023-12-13 02:37:29.752912 Epoch 110, Training loss 0.9314167376826791\n",
            "2023-12-13 02:38:00.889449 Epoch 112, Training loss 0.9326527790187875\n",
            "2023-12-13 02:38:32.058022 Epoch 114, Training loss 0.9323752921865419\n",
            "2023-12-13 02:39:03.499201 Epoch 116, Training loss 0.9262711654996019\n",
            "2023-12-13 02:39:34.542853 Epoch 118, Training loss 0.9244761245939738\n",
            "2023-12-13 02:40:06.080698 Epoch 120, Training loss 0.9211620124404692\n",
            "2023-12-13 02:40:36.801715 Epoch 122, Training loss 0.9250024631810005\n",
            "2023-12-13 02:41:07.926386 Epoch 124, Training loss 0.9202150091185899\n",
            "2023-12-13 02:41:38.635810 Epoch 126, Training loss 0.9218894508488648\n",
            "2023-12-13 02:42:10.200334 Epoch 128, Training loss 0.9186308590499946\n",
            "2023-12-13 02:42:40.866173 Epoch 130, Training loss 0.9160733636840225\n",
            "2023-12-13 02:43:11.879955 Epoch 132, Training loss 0.9153447683967287\n",
            "2023-12-13 02:43:42.432394 Epoch 134, Training loss 0.9139219961126747\n",
            "2023-12-13 02:44:13.820580 Epoch 136, Training loss 0.9112066096814392\n",
            "2023-12-13 02:44:45.248778 Epoch 138, Training loss 0.9135180046338864\n",
            "2023-12-13 02:45:16.086300 Epoch 140, Training loss 0.907941240826836\n",
            "2023-12-13 02:45:47.517888 Epoch 142, Training loss 0.9077292112133387\n",
            "2023-12-13 02:46:18.127771 Epoch 144, Training loss 0.9055197132213036\n",
            "2023-12-13 02:46:49.341383 Epoch 146, Training loss 0.9093218020465977\n",
            "2023-12-13 02:47:19.919347 Epoch 148, Training loss 0.9090534594205334\n",
            "2023-12-13 02:47:50.689409 Epoch 150, Training loss 0.9062661325077876\n",
            "2023-12-13 02:48:20.553501 Epoch 152, Training loss 0.9037807200418408\n",
            "2023-12-13 02:48:51.105039 Epoch 154, Training loss 0.9017741518938328\n",
            "2023-12-13 02:49:21.924751 Epoch 156, Training loss 0.9037639596273223\n",
            "2023-12-13 02:49:53.364100 Epoch 158, Training loss 0.9035707503328543\n",
            "2023-12-13 02:50:24.299861 Epoch 160, Training loss 0.8984877232395475\n",
            "2023-12-13 02:50:55.543313 Epoch 162, Training loss 0.9062740426234273\n",
            "2023-12-13 02:51:26.260605 Epoch 164, Training loss 0.8987431311241502\n",
            "2023-12-13 02:51:57.649520 Epoch 166, Training loss 0.8983734079334132\n",
            "2023-12-13 02:52:28.945904 Epoch 168, Training loss 0.8944341486982067\n",
            "2023-12-13 02:53:00.169241 Epoch 170, Training loss 0.9009031472761003\n",
            "2023-12-13 02:53:31.642754 Epoch 172, Training loss 0.8944177877186509\n",
            "2023-12-13 02:54:02.370901 Epoch 174, Training loss 0.8962729569057675\n",
            "2023-12-13 02:54:33.529578 Epoch 176, Training loss 0.8928667131592246\n",
            "2023-12-13 02:55:03.992520 Epoch 178, Training loss 0.8934577325420916\n",
            "2023-12-13 02:55:35.401477 Epoch 180, Training loss 0.896360237656347\n",
            "2023-12-13 02:56:06.068206 Epoch 182, Training loss 0.8932140632663541\n",
            "2023-12-13 02:56:37.644648 Epoch 184, Training loss 0.8941488764475068\n",
            "2023-12-13 02:57:08.392045 Epoch 186, Training loss 0.8916813163348781\n",
            "2023-12-13 02:57:40.290697 Epoch 188, Training loss 0.8904792626038232\n",
            "2023-12-13 02:58:11.341224 Epoch 190, Training loss 0.892257200253894\n",
            "2023-12-13 02:58:42.638827 Epoch 192, Training loss 0.8896427557748907\n",
            "2023-12-13 02:59:14.082286 Epoch 194, Training loss 0.8859737944953582\n",
            "2023-12-13 02:59:45.526258 Epoch 196, Training loss 0.8860925157814075\n",
            "2023-12-13 03:00:17.129804 Epoch 198, Training loss 0.8919637291251546\n",
            "2023-12-13 03:00:47.992505 Epoch 200, Training loss 0.8877474010524238\n",
            "2023-12-13 03:01:19.539501 Epoch 202, Training loss 0.8891041876409974\n",
            "2023-12-13 03:01:50.717266 Epoch 204, Training loss 0.890968679695788\n",
            "2023-12-13 03:02:21.977415 Epoch 206, Training loss 0.8872550257179134\n",
            "2023-12-13 03:02:53.297782 Epoch 208, Training loss 0.8890123987365561\n",
            "2023-12-13 03:03:24.094091 Epoch 210, Training loss 0.8867507905453977\n",
            "2023-12-13 03:03:55.621266 Epoch 212, Training loss 0.8896598377267418\n",
            "2023-12-13 03:04:26.627826 Epoch 214, Training loss 0.8837164722745071\n",
            "2023-12-13 03:04:57.932669 Epoch 216, Training loss 0.8826302796068703\n",
            "2023-12-13 03:05:28.707677 Epoch 218, Training loss 0.8836673886117423\n",
            "2023-12-13 03:06:00.730758 Epoch 220, Training loss 0.88256586588862\n",
            "2023-12-13 03:06:31.620930 Epoch 222, Training loss 0.883366018076382\n",
            "2023-12-13 03:07:03.150861 Epoch 224, Training loss 0.8778280743857478\n",
            "2023-12-13 03:07:34.489462 Epoch 226, Training loss 0.8814392183214197\n",
            "2023-12-13 03:08:05.685480 Epoch 228, Training loss 0.8768195990101456\n",
            "2023-12-13 03:08:37.009562 Epoch 230, Training loss 0.8793685193104512\n",
            "2023-12-13 03:09:08.082342 Epoch 232, Training loss 0.8843283398681895\n",
            "2023-12-13 03:09:38.964470 Epoch 234, Training loss 0.8798399230708247\n",
            "2023-12-13 03:10:09.232074 Epoch 236, Training loss 0.8785436452578401\n",
            "2023-12-13 03:10:41.334750 Epoch 238, Training loss 0.880330309919689\n",
            "2023-12-13 03:11:12.095459 Epoch 240, Training loss 0.8764403220027914\n",
            "2023-12-13 03:11:43.612030 Epoch 242, Training loss 0.8740610744032409\n",
            "2023-12-13 03:12:14.159576 Epoch 244, Training loss 0.8848042330321144\n",
            "2023-12-13 03:12:45.770361 Epoch 246, Training loss 0.8758079654267986\n",
            "2023-12-13 03:13:16.435313 Epoch 248, Training loss 0.8807493495514326\n",
            "2023-12-13 03:13:47.684795 Epoch 250, Training loss 0.870151422441463\n",
            "2023-12-13 03:14:18.953014 Epoch 252, Training loss 0.8759219695235152\n",
            "2023-12-13 03:14:50.043512 Epoch 254, Training loss 0.8725208923639849\n",
            "2023-12-13 03:15:20.841699 Epoch 256, Training loss 0.8759182885555965\n",
            "2023-12-13 03:15:50.974930 Epoch 258, Training loss 0.8733868982709582\n",
            "2023-12-13 03:16:21.445619 Epoch 260, Training loss 0.875809255463388\n",
            "2023-12-13 03:16:51.245378 Epoch 262, Training loss 0.8760154927935442\n",
            "2023-12-13 03:17:21.104010 Epoch 264, Training loss 0.8749890133090641\n",
            "2023-12-13 03:17:51.642615 Epoch 266, Training loss 0.8777950769647613\n",
            "2023-12-13 03:18:21.774414 Epoch 268, Training loss 0.8723384239484587\n",
            "2023-12-13 03:18:51.902490 Epoch 270, Training loss 0.8751529784458677\n",
            "2023-12-13 03:19:22.328646 Epoch 272, Training loss 0.8753325185355019\n",
            "2023-12-13 03:19:52.475929 Epoch 274, Training loss 0.8689170444331815\n",
            "2023-12-13 03:20:22.464578 Epoch 276, Training loss 0.8741219698087029\n",
            "2023-12-13 03:20:52.826129 Epoch 278, Training loss 0.8711953948220938\n",
            "2023-12-13 03:21:22.661629 Epoch 280, Training loss 0.8712845031562668\n",
            "2023-12-13 03:21:53.289839 Epoch 282, Training loss 0.8706784398506975\n",
            "2023-12-13 03:22:23.498904 Epoch 284, Training loss 0.8678223182783102\n",
            "2023-12-13 03:22:53.990516 Epoch 286, Training loss 0.8734856017715181\n",
            "2023-12-13 03:23:23.918576 Epoch 288, Training loss 0.8684768996122852\n",
            "2023-12-13 03:23:54.494069 Epoch 290, Training loss 0.8658865569421398\n",
            "2023-12-13 03:24:24.919068 Epoch 292, Training loss 0.8717832633144106\n",
            "2023-12-13 03:24:55.734756 Epoch 294, Training loss 0.8689915835095183\n",
            "2023-12-13 03:25:25.683200 Epoch 296, Training loss 0.8684258495297883\n",
            "2023-12-13 03:25:56.035954 Epoch 298, Training loss 0.8704010163579146\n",
            "2023-12-13 03:26:25.967963 Epoch 300, Training loss 0.869590295397717\n",
            "time: 1h 17min 18s (started: 2023-12-13 02:09:07 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_dropout, predictions_dropout, expected_labels_dropout = validate(model_dropout, train_loader, val_loader)\n",
        "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
        "\n",
        "\n",
        "precision_dropout, recall_dropout = precision_score(predictions_dropout, expected_labels_dropout, average='macro'), recall_score(predictions_dropout, expected_labels_dropout, average='macro')\n",
        "cnf_matrix_dropout = confusion_matrix(predictions_dropout, expected_labels_dropout)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4o9gLZBIlac",
        "outputId": "b9fee818-b820-4e1e-de37-be81d8b0560e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.69\n",
            "Accuracy val: 0.61\n",
            "time: 16 s (started: 2023-12-13 03:26:25 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(predictions_dropout, expected_labels_dropout, target_names=class_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EGztG51JDeN",
        "outputId": "262bb566-f2bb-4e54-c9e1-d8484b4a8c27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane       0.71      0.62      0.66      1155\n",
            "  automobile       0.73      0.73      0.73      1008\n",
            "        bird       0.46      0.48      0.46       957\n",
            "         cat       0.40      0.42      0.41       949\n",
            "        deer       0.55      0.57      0.56       967\n",
            "         dog       0.52      0.52      0.52      1003\n",
            "        frog       0.65      0.68      0.67       955\n",
            "       horse       0.67      0.66      0.66      1006\n",
            "        ship       0.71      0.74      0.73       963\n",
            "       truck       0.69      0.67      0.68      1037\n",
            "\n",
            "    accuracy                           0.61     10000\n",
            "   macro avg       0.61      0.61      0.61     10000\n",
            "weighted avg       0.61      0.61      0.61     10000\n",
            "\n",
            "time: 48.1 ms (started: 2023-12-13 03:26:42 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NetDropout(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.conv1_dropout = nn.Dropout2d(p=0.3)\n",
        "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
        "        self.conv2_dropout = nn.Dropout2d(p=0.3)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(8 * 8 * n_chans1 // 2, 32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = self.conv1_dropout(out)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = self.conv2_dropout(out)\n",
        "        return self.fc(out.view(-1, 8 * 8 * self.n_chans1 // 2))\n"
      ],
      "metadata": {
        "id": "cay0WplB0Bv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dropout = NetDropout(n_chans1=32).to(device=device)\n",
        "optimizer_dropout = optim.SGD(model_dropout.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 150,\n",
        "    optimizer = optimizer_dropout,\n",
        "    model = model_dropout,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZWtdoUT0E2X",
        "outputId": "06f7a7db-a525-4cb7-b3ba-461654b38b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-13 03:30:19.491158 Epoch 1, Training loss 1.995631134571017\n",
            "2023-12-13 03:30:35.413092 Epoch 2, Training loss 1.7166142004835026\n",
            "2023-12-13 03:31:04.297918 Epoch 4, Training loss 1.5339118175189514\n",
            "2023-12-13 03:31:33.365952 Epoch 6, Training loss 1.4455803617492051\n",
            "2023-12-13 03:32:02.049553 Epoch 8, Training loss 1.3710254606078653\n",
            "2023-12-13 03:32:30.829585 Epoch 10, Training loss 1.3151189550719298\n",
            "2023-12-13 03:32:59.980428 Epoch 12, Training loss 1.2685180420765791\n",
            "2023-12-13 03:33:29.915862 Epoch 14, Training loss 1.2409061988448853\n",
            "2023-12-13 03:33:59.286316 Epoch 16, Training loss 1.2171496484438171\n",
            "2023-12-13 03:34:29.038043 Epoch 18, Training loss 1.1916198180154767\n",
            "2023-12-13 03:34:58.821512 Epoch 20, Training loss 1.172063709982216\n",
            "2023-12-13 03:35:28.510125 Epoch 22, Training loss 1.1587711659538777\n",
            "2023-12-13 03:35:57.758815 Epoch 24, Training loss 1.1409421938154705\n",
            "2023-12-13 03:36:27.770373 Epoch 26, Training loss 1.1297474248939767\n",
            "2023-12-13 03:36:58.094917 Epoch 28, Training loss 1.1155457707774608\n",
            "2023-12-13 03:37:27.215639 Epoch 30, Training loss 1.1077896208714342\n",
            "2023-12-13 03:37:55.866693 Epoch 32, Training loss 1.091711281967895\n",
            "2023-12-13 03:38:24.240727 Epoch 34, Training loss 1.0799145126891563\n",
            "2023-12-13 03:38:53.045842 Epoch 36, Training loss 1.0700668506610118\n",
            "2023-12-13 03:39:21.665452 Epoch 38, Training loss 1.0679707087366783\n",
            "2023-12-13 03:39:49.944431 Epoch 40, Training loss 1.05682726863705\n",
            "2023-12-13 03:40:18.157507 Epoch 42, Training loss 1.051748623476004\n",
            "2023-12-13 03:40:47.066162 Epoch 44, Training loss 1.0432288443188533\n",
            "2023-12-13 03:41:17.102222 Epoch 46, Training loss 1.0330119452360647\n",
            "2023-12-13 03:41:46.195222 Epoch 48, Training loss 1.0300502928016741\n",
            "2023-12-13 03:42:14.828610 Epoch 50, Training loss 1.0268050081589644\n",
            "2023-12-13 03:42:43.763444 Epoch 52, Training loss 1.0218557578218563\n",
            "2023-12-13 03:43:12.425817 Epoch 54, Training loss 1.0149570116606514\n",
            "2023-12-13 03:43:41.124458 Epoch 56, Training loss 1.012393340201634\n",
            "2023-12-13 03:44:09.774090 Epoch 58, Training loss 1.007109241641086\n",
            "2023-12-13 03:44:38.278793 Epoch 60, Training loss 1.0003221958799435\n",
            "2023-12-13 03:45:06.887261 Epoch 62, Training loss 1.0001492302131165\n",
            "2023-12-13 03:45:35.875680 Epoch 64, Training loss 0.9911128242912195\n",
            "2023-12-13 03:46:04.564183 Epoch 66, Training loss 0.9895966652104312\n",
            "2023-12-13 03:46:33.065458 Epoch 68, Training loss 0.9856957736832407\n",
            "2023-12-13 03:47:01.748525 Epoch 70, Training loss 0.980950797838933\n",
            "2023-12-13 03:47:30.349647 Epoch 72, Training loss 0.983398488949022\n",
            "2023-12-13 03:47:59.020660 Epoch 74, Training loss 0.9726212825006841\n",
            "2023-12-13 03:48:27.316634 Epoch 76, Training loss 0.9698261918924044\n",
            "2023-12-13 03:48:56.085065 Epoch 78, Training loss 0.9694530550781113\n",
            "2023-12-13 03:49:24.361443 Epoch 80, Training loss 0.9659042766941782\n",
            "2023-12-13 03:49:53.417540 Epoch 82, Training loss 0.965613585756258\n",
            "2023-12-13 03:50:22.451910 Epoch 84, Training loss 0.96484386334029\n",
            "2023-12-13 03:50:51.455101 Epoch 86, Training loss 0.9591541963312632\n",
            "2023-12-13 03:51:20.304674 Epoch 88, Training loss 0.9605427652673648\n",
            "2023-12-13 03:51:48.765503 Epoch 90, Training loss 0.9516667997288277\n",
            "2023-12-13 03:52:17.667525 Epoch 92, Training loss 0.9521469899150722\n",
            "2023-12-13 03:52:46.653846 Epoch 94, Training loss 0.9477140970547181\n",
            "2023-12-13 03:53:15.468834 Epoch 96, Training loss 0.9475542133878869\n",
            "2023-12-13 03:53:44.496995 Epoch 98, Training loss 0.9441006540337487\n",
            "2023-12-13 03:54:13.629874 Epoch 100, Training loss 0.9420907517985615\n",
            "2023-12-13 03:54:42.932571 Epoch 102, Training loss 0.941798904972613\n",
            "2023-12-13 03:55:11.946615 Epoch 104, Training loss 0.9430176928220197\n",
            "2023-12-13 03:55:40.841116 Epoch 106, Training loss 0.9365395677211644\n",
            "2023-12-13 03:56:09.418416 Epoch 108, Training loss 0.9346019974754899\n",
            "2023-12-13 03:56:39.494603 Epoch 110, Training loss 0.934977188210963\n",
            "2023-12-13 03:57:08.080421 Epoch 112, Training loss 0.9361751066601794\n",
            "2023-12-13 03:57:36.893573 Epoch 114, Training loss 0.9363617684377734\n",
            "2023-12-13 03:58:05.785838 Epoch 116, Training loss 0.9342280558460508\n",
            "2023-12-13 03:58:34.935650 Epoch 118, Training loss 0.9318778668828023\n",
            "2023-12-13 03:59:03.512400 Epoch 120, Training loss 0.9238594967081114\n",
            "2023-12-13 03:59:32.237245 Epoch 122, Training loss 0.9237755647553202\n",
            "2023-12-13 04:00:00.943337 Epoch 124, Training loss 0.9204273082106315\n",
            "2023-12-13 04:00:29.698213 Epoch 126, Training loss 0.9217516915572573\n",
            "2023-12-13 04:00:58.158769 Epoch 128, Training loss 0.9267478428228432\n",
            "2023-12-13 04:01:26.429494 Epoch 130, Training loss 0.9276723258788019\n",
            "2023-12-13 04:01:54.749877 Epoch 132, Training loss 0.9226402279056246\n",
            "2023-12-13 04:02:23.388303 Epoch 134, Training loss 0.9177462459372743\n",
            "2023-12-13 04:02:51.904930 Epoch 136, Training loss 0.9197948538433866\n",
            "2023-12-13 04:03:20.703868 Epoch 138, Training loss 0.9243319560499752\n",
            "2023-12-13 04:03:49.828400 Epoch 140, Training loss 0.9194747663824759\n",
            "2023-12-13 04:04:18.135651 Epoch 142, Training loss 0.9199853742976323\n",
            "2023-12-13 04:04:46.489420 Epoch 144, Training loss 0.919056126757351\n",
            "2023-12-13 04:05:14.998440 Epoch 146, Training loss 0.9216989137784904\n",
            "2023-12-13 04:05:43.466845 Epoch 148, Training loss 0.9102556106379575\n",
            "2023-12-13 04:06:11.768192 Epoch 150, Training loss 0.9121346282379706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_dropout, predictions_dropout, expected_labels_dropout = validate(model_dropout, train_loader, val_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEKmrEch1I1b",
        "outputId": "f13a5019-cdbe-4937-94f7-5a6f7cde0a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.68\n",
            "Accuracy val: 0.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "precision_dropout, recall_dropout, cnf_matrix_dropout = precision_score(predictions_dropout, expected_labels_dropout, average='macro'), recall_score(predictions_dropout, expected_labels_dropout, average='macro'), confusion_matrix(predictions_dropout, expected_labels_dropout)\n"
      ],
      "metadata": {
        "id": "NIUb6Avs1MzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(predictions_dropout, expected_labels_dropout, target_names=class_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRdTOmC_3EMI",
        "outputId": "c798798d-8a16-4e9f-c21a-4767e9758576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane       0.70      0.66      0.68      1053\n",
            "  automobile       0.77      0.72      0.74      1063\n",
            "        bird       0.48      0.52      0.50       923\n",
            "         cat       0.44      0.45      0.44       988\n",
            "        deer       0.54      0.54      0.54      1001\n",
            "         dog       0.54      0.53      0.53      1030\n",
            "        frog       0.70      0.70      0.70       991\n",
            "       horse       0.63      0.67      0.65       939\n",
            "        ship       0.76      0.72      0.74      1058\n",
            "       truck       0.68      0.71      0.69       954\n",
            "\n",
            "    accuracy                           0.62     10000\n",
            "   macro avg       0.62      0.62      0.62     10000\n",
            "weighted avg       0.63      0.62      0.62     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NetBatchNorm(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_chans1)\n",
        "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
        "        self.conv2_batchnorm = nn.BatchNorm2d(num_features=n_chans1 // 2)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(8 * 8 * n_chans1 // 2, 32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1_batchnorm(self.conv1(x))), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2_batchnorm(self.conv2(out))), 2)\n",
        "        return self.fc(out.view(-1, 8 * 8 * self.n_chans1 // 2))\n"
      ],
      "metadata": {
        "id": "KGSfn4Hm3FE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_batch_norm = NetBatchNorm(n_chans1=32).to(device=device)\n",
        "optimizer_batch_norm = optim.SGD(model_batch_norm.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer_batch_norm,\n",
        "    model = model_batch_norm,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY6Dxzjg4iQG",
        "outputId": "433766c4-9e65-4e91-b69f-2839fc7867fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-13 04:07:30.870113 Epoch 1, Training loss 1.807895163898273\n",
            "2023-12-13 04:07:45.275596 Epoch 2, Training loss 1.482054467396358\n",
            "2023-12-13 04:08:13.294459 Epoch 4, Training loss 1.2449691876426072\n",
            "2023-12-13 04:08:41.692687 Epoch 6, Training loss 1.113484408087133\n",
            "2023-12-13 04:09:10.446882 Epoch 8, Training loss 1.0256894990001493\n",
            "2023-12-13 04:09:39.203947 Epoch 10, Training loss 0.9600044155059873\n",
            "2023-12-13 04:10:07.580682 Epoch 12, Training loss 0.9097343064330118\n",
            "2023-12-13 04:10:36.053742 Epoch 14, Training loss 0.8703683008775687\n",
            "2023-12-13 04:11:04.069054 Epoch 16, Training loss 0.8380810820385624\n",
            "2023-12-13 04:11:32.342050 Epoch 18, Training loss 0.8104771412242099\n",
            "2023-12-13 04:12:00.744576 Epoch 20, Training loss 0.7861267812645344\n",
            "2023-12-13 04:12:28.729499 Epoch 22, Training loss 0.7642831300835475\n",
            "2023-12-13 04:12:57.045642 Epoch 24, Training loss 0.7443065306033625\n",
            "2023-12-13 04:13:25.116664 Epoch 26, Training loss 0.7255052128411315\n",
            "2023-12-13 04:13:53.047694 Epoch 28, Training loss 0.7078595272720317\n",
            "2023-12-13 04:14:20.985292 Epoch 30, Training loss 0.6912712748626919\n",
            "2023-12-13 04:14:49.092350 Epoch 32, Training loss 0.6757075488948456\n",
            "2023-12-13 04:15:17.268358 Epoch 34, Training loss 0.6608098785361975\n",
            "2023-12-13 04:15:45.508473 Epoch 36, Training loss 0.6466318036208067\n",
            "2023-12-13 04:16:14.322349 Epoch 38, Training loss 0.6328793518302386\n",
            "2023-12-13 04:16:42.723924 Epoch 40, Training loss 0.619676739244205\n",
            "2023-12-13 04:17:11.069955 Epoch 42, Training loss 0.6069353775066488\n",
            "2023-12-13 04:17:39.044893 Epoch 44, Training loss 0.594603138316013\n",
            "2023-12-13 04:18:07.425114 Epoch 46, Training loss 0.5826252596762479\n",
            "2023-12-13 04:18:35.406105 Epoch 48, Training loss 0.571035965248142\n",
            "2023-12-13 04:19:03.434635 Epoch 50, Training loss 0.5598086693783855\n",
            "2023-12-13 04:19:31.131676 Epoch 52, Training loss 0.5489840739980683\n",
            "2023-12-13 04:19:58.947547 Epoch 54, Training loss 0.5383334288664181\n",
            "2023-12-13 04:20:26.585551 Epoch 56, Training loss 0.5279206594314112\n",
            "2023-12-13 04:20:54.243970 Epoch 58, Training loss 0.5178152740459003\n",
            "2023-12-13 04:21:21.861267 Epoch 60, Training loss 0.5078171799935953\n",
            "2023-12-13 04:21:49.519566 Epoch 62, Training loss 0.49788735496342335\n",
            "2023-12-13 04:22:17.472215 Epoch 64, Training loss 0.48834155221729325\n",
            "2023-12-13 04:22:45.623510 Epoch 66, Training loss 0.478839203277055\n",
            "2023-12-13 04:23:14.955861 Epoch 68, Training loss 0.4696472422660464\n",
            "2023-12-13 04:23:43.352991 Epoch 70, Training loss 0.4605398171431268\n",
            "2023-12-13 04:24:11.166264 Epoch 72, Training loss 0.4515608242519981\n",
            "2023-12-13 04:24:39.241674 Epoch 74, Training loss 0.44278179253915995\n",
            "2023-12-13 04:25:07.276634 Epoch 76, Training loss 0.43409003616522646\n",
            "2023-12-13 04:25:34.956973 Epoch 78, Training loss 0.4255599257014597\n",
            "2023-12-13 04:26:02.708185 Epoch 80, Training loss 0.41718492163416676\n",
            "2023-12-13 04:26:30.585201 Epoch 82, Training loss 0.40902636129685377\n",
            "2023-12-13 04:26:58.962824 Epoch 84, Training loss 0.4009479667676989\n",
            "2023-12-13 04:27:26.831093 Epoch 86, Training loss 0.39312418374945135\n",
            "2023-12-13 04:27:55.032824 Epoch 88, Training loss 0.38546601969682043\n",
            "2023-12-13 04:28:23.068207 Epoch 90, Training loss 0.3779647819259587\n",
            "2023-12-13 04:28:51.120946 Epoch 92, Training loss 0.3706416390035921\n",
            "2023-12-13 04:29:19.801855 Epoch 94, Training loss 0.36352535718313567\n",
            "2023-12-13 04:29:48.437414 Epoch 96, Training loss 0.35645956734714607\n",
            "2023-12-13 04:30:16.938652 Epoch 98, Training loss 0.3496372926303798\n",
            "2023-12-13 04:30:45.118789 Epoch 100, Training loss 0.34283076516350214\n"
          ]
        }
      ]
    }
  ]
}